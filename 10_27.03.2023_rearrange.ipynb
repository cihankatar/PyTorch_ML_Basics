{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IMPORT \n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 15, 20, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "# suppose we have a set of 32 images in \"h w c\" format (height-width-channel)\n",
    "images = [np.random.randn(30, 40, 3) for _ in range(32)]\n",
    "\n",
    "# stack along first (batch) axis, output is a single array\n",
    "rearrange(images, 'b h w c -> b h w c').shape\n",
    "(32, 30, 40, 3)\n",
    "\n",
    "# concatenate images along height (vertical axis), 960 = 32 * 30\n",
    "rearrange(images, 'b h w c -> (b h) w c').shape\n",
    "(960, 40, 3)\n",
    "\n",
    "# concatenated images along horizontal axis, 1280 = 32 * 40\n",
    "rearrange(images, 'b h w c -> h (b w) c').shape\n",
    "(30, 1280, 3)\n",
    "\n",
    "# reordered axes to \"b c h w\" format for deep learning\n",
    "rearrange(images, 'b h w c -> b c h w').shape\n",
    "(32, 3, 30, 40)\n",
    "\n",
    "# flattened each image into a vector, 3600 = 30 * 40 * 3\n",
    "rearrange(images, 'b h w c -> b (c h w)').shape\n",
    "(32, 3600)\n",
    "\n",
    "# split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2\n",
    "rearrange(images, 'b (h1 h) (w1 w) c -> (b h1 w1) h w c', h1=2, w1=2).shape\n",
    "(128, 15, 20, 3)\n",
    "\n",
    "# space-to-depth operation\n",
    "rearrange(images, 'b (h h1) (w w1) c -> b h w (c h1 w1)', h1=2, w1=2).shape\n",
    "(32, 15, 20, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 65, 512])\n",
      "torch.Size([1, 4, 65, 65])\n"
     ]
    }
   ],
   "source": [
    "qkv=torch.rand(1,65,6144)\n",
    "head_num=4\n",
    "\n",
    "query, key, value = tuple(rearrange(qkv, 'b t (d k h ) -> k b h t d ', k=3, h=head_num))\n",
    "print(value.shape)\n",
    "\n",
    "energy = torch.einsum(\"... i d , ... j d -> ... i j\", query, key) \n",
    "print(energy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[3, 1],\n",
      "          [7, 2],\n",
      "          [4, 4]],\n",
      "\n",
      "         [[2, 5],\n",
      "          [4, 3],\n",
      "          [1, 5]],\n",
      "\n",
      "         [[6, 4],\n",
      "          [1, 3],\n",
      "          [7, 4]],\n",
      "\n",
      "         [[3, 2],\n",
      "          [3, 1],\n",
      "          [5, 3]]]]) torch.Size([1, 4, 3, 2])\n",
      "tensor([[[[10, 23, 16],\n",
      "          [23, 53, 36],\n",
      "          [16, 36, 32]],\n",
      "\n",
      "         [[29, 23, 27],\n",
      "          [23, 25, 19],\n",
      "          [27, 19, 26]],\n",
      "\n",
      "         [[52, 18, 58],\n",
      "          [18, 10, 19],\n",
      "          [58, 19, 65]],\n",
      "\n",
      "         [[13, 11, 21],\n",
      "          [11, 10, 18],\n",
      "          [21, 18, 34]]]]) torch.Size([1, 4, 3, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/cihankatar/Desktop/Github_Repo/PyTorch_Practising/10_27.03.2023_rearrange.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cihankatar/Desktop/Github_Repo/PyTorch_Practising/10_27.03.2023_rearrange.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(rand_tensor,rand_tensor\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cihankatar/Desktop/Github_Repo/PyTorch_Practising/10_27.03.2023_rearrange.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(energy,energy\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cihankatar/Desktop/Github_Repo/PyTorch_Practising/10_27.03.2023_rearrange.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m torch\u001b[39m.\u001b[39;49mmatmul(rand_tensor, rand_tensor\u001b[39m.\u001b[39;49mT)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "rand_tensor     =   torch.tensor([[[[3,1],\n",
    "                                    [7,2],\n",
    "                                    [4,4]],\n",
    "                                   [[2,5],\n",
    "                                    [4,3],\n",
    "                                    [1,5]],\n",
    "                                   [[6,4],\n",
    "                                    [1,3],\n",
    "                                    [7,4]],\n",
    "                                   [[3,2],\n",
    "                                    [3,1],\n",
    "                                    [5,3]]\n",
    "                                    ]])\n",
    "\n",
    "energy = torch.einsum(\"... i d , ... j d -> ... i j\", rand_tensor, rand_tensor) \n",
    "print(rand_tensor,rand_tensor.shape)\n",
    "print(energy,energy.shape)\n",
    "torch.matmul(rand_tensor, rand_tensor.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10, 23, 16],\n",
       "        [23, 53, 36],\n",
       "        [16, 36, 32]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_tensor=torch.tensor([[3, 1],\n",
    "                [7, 2],\n",
    "                [4, 4]])\n",
    "print(rand_tensor.shape)\n",
    "torch.matmul(rand_tensor, rand_tensor.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
