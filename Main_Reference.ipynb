{"cells":[{"cell_type":"markdown","metadata":{"id":"pNHUWmFNDzYK"},"source":["**How to solve too many changes were detected problem in VScode**\n","\n","[The Git repository has too many active changes](https://www.youtube.com/watch?v=uFStXMyhSJI)\n","\n","https://code.visualstudio.com/docs/sourcecontrol/overview (Good link to start)"]},{"cell_type":"markdown","metadata":{"id":"NgLF2C8eDu-q"},"source":[]},{"cell_type":"markdown","metadata":{"id":"1cK5idrzDm8X"},"source":[]},{"cell_type":"markdown","metadata":{"id":"laG_Cs_s1T51"},"source":["**GIT instalation on MAC**\n","\n","1-) Instal brew on your MAC if you don't already have it. Go (https://brew.sh/) Run the installation code on the terminal.\n","\n","2-) Run $ *brew install git* \n","\n","command on the terminal.\n","\n","**Use Git with VScode**\n","\n","3-) Watch the tutorial (https://www.youtube.com/watch?v=i_23KUAEtUM)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3858,"status":"ok","timestamp":1672230635838,"user":{"displayName":"Cihan Katar","userId":"05207696428297684225"},"user_tz":-180},"id":"_hx7mJuDRBYJ"},"outputs":[],"source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torchvision\n","from torchvision.datasets import MNIST\n","import torchvision.transforms as transforms\n","import numpy as  np\n","import torch.nn.functional as F\n","import torch.nn as nn "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1667641198706,"user":{"displayName":"Cihan Katar","userId":"05207696428297684225"},"user_tz":-180},"id":"NgBCSNqbnTHh","outputId":"e9d849b9-1c94-4208-ddbd-8870c277231f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[19, 22],\n","        [43, 50]])\n","tensor([[ 5, 12],\n","        [21, 32]])\n","tensor([[19, 22],\n","        [43, 50]])\n"]}],"source":["a = torch.tensor([[1,2],\n","                  [3,4]])\n","b = torch.tensor([[5,6],\n","                  [7,8]])\n","c = a@b #For dot product\n","print(c)\n","\n","d = a*b #For elementwise multiplication \n","print(d)\n","print(c)"]},{"cell_type":"markdown","metadata":{"id":"SGqoTMUZQ0HN"},"source":["# ***BASICS***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aV68s7fdQytn"},"outputs":[],"source":["import torch\n","a=torch.tensor([[5,9],\n","                [2,6],\n","                [2,3]]) # 3x2 \n","\n","b=torch.rand(3,2)\n","x=torch.tensor([[5,9,4],\n","                [2,6,5],\n","                      ]) # 2x3 \n","\n","elementwise = a*b    # element wise multiplication =3x2\n","dotproduct  = a@x     # dot product     3x2  2x3 =3x3\n","print(f'\\n elementwise a,b \\n',elementwise)\n","print(f'\\n dotproduct a,x \\n=',dotproduct)\n","\n","c=torch.cat((a,b),dim=0)\n","print(f'concatinating a and b in axis 0\\n',c)\n","d=torch.cat((a,b),dim=1)\n","print(f'\\nconcatinating a and b in axis 1\\n',d)\n","\n","e= a[:,1]   # tensor([9, 6, 3])\n","print(f'\\nslicing a[:,1] = ',e)\n","\n","f= a[1,:]   # tensor([2,6])\n","print('\\nslicing a[1,:] = {}'.format({f}))\n","\n","sum=a.sum()\n","print(f'\\nsum =', sum)\n","\n","sum_axis_keep_dim=a.sum(dim=1,keepdim=True)\n","sum_axis=a.sum(dim=0)\n","print(f'\\nsum and keep dim =',sum_axis)\n","print(f'\\nsum and keep dim =',sum_axis_keep_dim)\n","\n","#Converting \n","convert_numpy = a.numpy()\n","convert_torch = torch.from_numpy(convert_numpy)\n","type(convert_numpy), type(convert_torch)\n","\n","k=torch.arange(10)\n","print(f\"k=\",k)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"h2S2HVQ3z7J8"},"source":["# Math operations\n","\n","In Python, you use the double slash **//** operator to perform floor division. This **//** operator divides the first number by the second number and rounds the result down to the nearest integer (or whole number)"]},{"cell_type":"markdown","metadata":{"id":"Up70QwQnTlHY"},"source":["# **BROADCASTING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gV0CUqLnTm7S"},"outputs":[],"source":["import torchvision.transforms.functional as F\n","\n","a=torch.tensor([[5,4],\n","                [2,3],\n","                [3,4]]) # 3x2\n","b=torch.tensor([1,2])   # 1x2\n","print(a+b)\n","print(a[-1],b[-1])\n","\n","print(f\"memroy adress of a    :\",id(a))\n","a=a+b     #saves different memory\n","print(f\"memroy adress of a+b  :\",id(a))\n","\n","a+=b      #saves same memory\n","print(f\"memroy adress of a+=b :\",id(a))\n","a[:]=a+b  \n","print(f\"memroy adress of a[:] :\",id(a))\n","\n","t = torch.randn(2,2,2)\n","t_resized = F.resize(t, 4)\n","\n","print(f\"t :\",t.shape)                 \n","print(f\"resize of t :\",t_resized.shape)\n","\n","\n","print(f\"\\nt :\",t)\n","print(f\"\\nresize of t :\",t_resized)\n","\n","t_flatten = t.view(2, -1) \n","print(f\"\\nt_flatten.shape :\",t_flatten)\n","print(t_flatten.shape)"]},{"cell_type":"markdown","metadata":{"id":"aoBH6HaDOajB"},"source":["# **ZIP**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XbeVTlag5_vY"},"outputs":[],"source":["import numpy as np\n","Class_numbers=np.array(['a','b','c'])\n","students_per_class=np.array([10,20,30])\n","print(\"Students counts per class:\\n{}\".format(\n","{x: y for x, y in zip(Class_numbers, students_per_class)}))"]},{"cell_type":"markdown","metadata":{"id":"mRkflC7cOd74"},"source":["# **LIST COMPREHENSÄ°ON**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7vI0G7V86cK"},"outputs":[],"source":["# Empty list\n","List = []\n","# Traditional approach of iterating\n","for character in 'Geeks 4 Geeks!':\n","    List.append(character) \n","# Display list\n","print(List)\n","\n","# Using list comprehension to iterate through loop\n","List = [character for character in 'Geeks 4 Geeks!']\n","# Displaying list\n","print(List)\n","\n","# ---------\n","list = [i for i in range(11) if i % 2 == 0]\n","print(list)\n","\n","# ---------\n","\n","matrix = [] \n","for i in range(3):\n","    # Append an empty sublist inside the list\n","    matrix.append([])\n","    for j in range(5):\n","        matrix[i].append(j)\n","print(matrix)\n","\n","# Nested list comprehension\n","matrix = [[j for j in range(5)] for i in range(3)]\n","print(matrix)"]},{"cell_type":"markdown","metadata":{"id":"szn05VJRYhwF"},"source":["# **Sequezee and Unsequeeze**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qcxtELQYgGY"},"outputs":[],"source":["# 1. (un)squeeze tensor\n","import torch\n","\n","y = torch.tensor([[[1, 2, 3, 4]]])\n","print(y.shape)\n","y = y.squeeze(1)\n","print(y.shape)\n","\n","y = torch.tensor([1, 2, 3, 4])\n","print(y.shape)\n","y = y.unsqueeze(1)\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"id":"f2t_XZT4aoaI"},"source":["# **MASKING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzH-7zrDaoKQ"},"outputs":[],"source":["import torch\n","tensor = torch.tensor([\n","    [1, 2, 3, 4, 5],\n","    [6, 7, 8, 9, 10],\n","    [11, 12, 13, 14, 15]\n","])\n","# Full mask\n","mask = torch.tensor([\n","    [1, 1, 0, 1, 1],\n","    [1, 1, 0, 1, 1],\n","    [1, 1, 0, 1, 1]\n","])\n","print(tensor)\n","x = torch.masked_fill(tensor, mask, value=9999)\n","print(x)\n","# Row broadcasting\n","mask = torch.tensor([[1, 1, 0, 1, 1]])\n","print(tensor)\n","y = torch.masked_fill(tensor, mask, value=9999)\n","print(y)\n","\n","# Column broadcasting\n","mask = torch.tensor([[1],[0],[0]])\n","\n","print(tensor)\n","z = torch.masked_fill(tensor, mask, value=9999)\n","print(z)"]},{"cell_type":"markdown","metadata":{"id":"8OwBkNVZx6lE"},"source":["# **ITERATOR OBJECT**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XESeJfHWsWZt"},"outputs":[],"source":["# list of vowels\n","vowels = [\"a\", \"e\", \"i\", \"o\", \"u\"]\n","\n","# iter() with a list of vowels\n","vowels_iter = iter(vowels)\n","\n","print(next(vowels_iter))\n","print(next(vowels_iter))\n","print(next(vowels_iter))\n","print(next(vowels_iter))\n","print(next(vowels_iter))\n"]},{"cell_type":"markdown","metadata":{"id":"C5l1ZgyibvQk"},"source":["# **NN.MODULE**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1667641207099,"user":{"displayName":"Cihan Katar","userId":"05207696428297684225"},"user_tz":-180},"id":"cK48Ttp9bx3u","outputId":"cdf3e261-bc8e-4ca7-97b4-90e78478d319"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[0.5027, 0.2909],\n","         [0.1840, 0.6697]],\n","\n","        [[0.2784, 0.0506],\n","         [0.0360, 0.2570]]])\n","torch.Size([2, 4])\n"]}],"source":["\n","import torch\n","from torch import nn\n","class NeuralNet(nn.Module):\n","    def __init__(self):\n","        super(NeuralNet, self).__init__()\n","        self.flatten = nn.Flatten()\n","    def forward(self, x):\n","        tensor = self.flatten(x)\n","        return tensor\n","\n","x = torch.rand(2, 2, 2)   # it does not work with 2D    2x2x2 = > 2x4\n","print(x) \n","model = NeuralNet()\n","output = model(x)\n","print(output.shape)"]},{"cell_type":"markdown","metadata":{"id":"8COScIESdztd"},"source":["# **Neural_Nets_Sequencial_and_Parameterlist**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8bfTeCTU0M9"},"outputs":[],"source":["#code:https://www.youtube.com/watch?v=C8Xr8-58HDs&list=PLLeO8f6PhlKb_FAC7qxOBtxT9-8EPDAqk&index=5\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","net=nn.Sequential(nn.Linear(2,4),nn.Linear(4,1))\n","X=torch.randn(3,2)\n","print(\"\\n\",net(X))\n","print(\"\\n\",net[0].weight.data)\n","print(\"\\n\",net[0].bias.data)\n","\n","for name, param in net.named_parameters():\n","  print(\"\\n\",param)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1667641208273,"user":{"displayName":"Cihan Katar","userId":"05207696428297684225"},"user_tz":-180},"id":"kfPTijFCd2LS","outputId":"f3a61d58-251b-45f6-b762-37cd40404735"},"outputs":[{"name":"stdout","output_type":"stream","text":["output size of 1 layer model with Linear: torch.Size([4, 2])\n","output size of N layer model with sequential and linear: torch.Size([4, 2])\n","output size of N layer mdoel with sequential and linear: torch.Size([4, 2])\n","torch.Size([10, 16])\n","torch.Size([10, 10])\n","torch.Size([10, 10])\n","torch.Size([10, 10])\n","torch.Size([2, 10])\n"]}],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","#------------\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(NeuralNet, self).__init__()\n","        self.nn = nn.Linear(input_size, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","        # print(self.nn.weight)  # Shape 2x16\n","        # print(self.nn.bias)  # shape: 2\n","\n","    def forward(self, inputs):\n","        tensor = self.nn(inputs)\n","        tensor = self.sigmoid(tensor)\n","        return tensor\n","        \n","x = torch.rand(4, 16)\n","model = NeuralNet(16, 2)\n","output = model(x)\n","print(f\"output size of 1 layer model with Linear:\",output.shape)\n","\n","# Sequential\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, output_size, num_hidden_layers=10, hidden_layer_size=128):\n","        super(NeuralNet, self).__init__()\n","        self.deep_nn = nn.Sequential()\n","        for i in range(num_hidden_layers):\n","            self.deep_nn.add_module(f'ff{i}', nn.Linear(input_size, hidden_layer_size))\n","            self.deep_nn.add_module(f'activation{i}', nn.ReLU())\n","            input_size = hidden_layer_size\n","        self.deep_nn.add_module(f'classifier', nn.Linear(hidden_layer_size, output_size))\n","\n","    def forward(self, inputs):\n","        tensor = self.deep_nn(inputs)\n","        return tensor\n","\n","x = torch.rand(4, 16)\n","model = NeuralNet(16, 2)\n","output = model(x)\n","print(f\"output size of N layer model with sequential and linear:\",output.shape)\n","\n","#Parameterlist\n","\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, output_size,\n","                 num_hidden_layers=4,\n","                 hidden_layer_size=10,\n","                 ):\n","        super(NeuralNet, self).__init__()\n","        self.activation = nn.ReLU()\n","        self.deep_nn = nn.ParameterList()\n","        for i in range(num_hidden_layers):\n","            self.deep_nn.append(nn.Parameter(torch.rand(hidden_layer_size, input_size)))\n","            input_size = hidden_layer_size\n","        self.deep_nn.append(nn.Parameter(torch.rand(output_size, input_size)))\n","\n","    def forward(self, inputs):\n","        hidden_states = []\n","        for idx, layer in enumerate(self.deep_nn):\n","            # Linear: `y = xA^T + b`\n","            tensor = F.linear(inputs, layer)\n","            if idx != len(self.deep_nn) - 1:\n","                tensor = self.activation(tensor)\n","            hidden_states.append(tensor)\n","            inputs = tensor\n","        return hidden_states[-1], hidden_states\n","\n","x = torch.rand(4, 16)\n","model = NeuralNet(16, 2)\n","output, states = model(x)\n","print(f\"output size of N layer mdoel with sequential and linear:\",output.shape)\n","for name, param in model.named_parameters():\n","  print(param.shape)"]},{"cell_type":"markdown","metadata":{"id":"Ck5bBaRrWItB"},"source":["# **Linear vs Parameter**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1667641210337,"user":{"displayName":"Cihan Katar","userId":"05207696428297684225"},"user_tz":-180},"id":"yRWM_t99VPZy","outputId":"87c44e3c-6ca9-41b4-d867-da35831aec31"},"outputs":[{"name":"stdout","output_type":"stream","text":["weights torch.Size([300, 10])\n","bias torch.Size([10])\n"]}],"source":["class ModelOne(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.weights = nn.Parameter(torch.randn(300, 10))\n","    self.bias = nn.Parameter(torch.zeros(10))\n","  def forward(self, x):\n","    return x @ self.weights + self.bias\n","\n","class ModelTwo(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.linear = nn.Linear(300, 10)\n","\n","  def forward(self, x):\n","    return self.linear(x)\n","'''    \n","The output in ModelOne:                   In ModelTwo:\n","weights torch.Size([300, 10])             linear.weight torch.Size([10, 300])\n","bias torch.Size([10])                     linear.bias torch.Size([10])\n","'''\n","model = ModelOne()\n","\n","[len(param) for param in model.parameters()] \n","for name, param in model.named_parameters():\n","    print(name, param.shape)"]},{"cell_type":"markdown","metadata":{"id":"fUTqqCUWgerK"},"source":["# **Activition_Function**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qa6G1ENfghyY"},"outputs":[],"source":["# Why do we need activation functions?\n","# 1. To add non-linearity to the network\n","# 2. To add stability in the training as in some cases the variance in the data is to high and\n","# using activations we can restrict them to a range {0, 1} or {-1, 1} etc.\n","# 3. Control the information flow in the network\n","# Commonly used: Sigmoid, Tanh, ReLU, LeakyReLU/PReLU, ELU\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","from matplotlib import pyplot as plt\n","x = torch.arange(start=-10, end=10, step=0.1)\n","# m = nn.Sigmoid() # m = nn.Tanh() # m = nn.ReLU() # m = nn.LeakyReLU()\n","m =  nn.ReLU()\n","m =  nn.GELU()\n","\n","m_out = m(x)\n","print(m_out)\n","\n","plt.plot(x.numpy(), m_out.numpy())\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"g8FkjNEIRCpq"},"source":["# **PLOT & SUBPLOT**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zP27d1Cd-jjA"},"outputs":[],"source":["#####\n","plt.plot([1, 2, 3, 4])\n","plt.ylabel('some numbers')\n","plt.show()\n","\n","######\n","plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')\n","plt.axis([0, 6, 0, 20])\n","plt.show()\n","\n","######\n","names = ['group_a', 'group_b', 'group_c']\n","values = [1, 10, 100]\n","\n","plt.figure(figsize=(9, 3))\n","\n","plt.subplot(131)\n","plt.bar(names, values)\n","plt.subplot(132)\n","plt.scatter(names, values)\n","plt.subplot(133)\n","plt.plot(names, values)\n","plt.suptitle('Categorical Plotting')\n","plt.show()\n","######\n","def f(t):\n","    return np.exp(-t) * np.cos(2*np.pi*t)\n","\n","t1 = np.arange(0.0, 5.0, 0.1)\n","t2 = np.arange(0.0, 5.0, 0.02)\n","\n","plt.figure()\n","plt.subplot(211)\n","plt.plot(t1, f(t1), 'bo', t2, f(t2), 'k')\n","\n","plt.subplot(212)\n","plt.plot(t2, np.cos(2*np.pi*t2), 'r--')\n","plt.show()\n","######\n","figure = plt.figure()\n","num_of_images = 10\n","for index in range(1, num_of_images + 1):\n","    plt.subplot(6, 10, index)\n","    plt.axis('off')\n","    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')"]},{"cell_type":"markdown","metadata":{"id":"Q_rl0UCgcT56"},"source":["# **MODULE_LIST**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrBK1UpfCpeq"},"outputs":[],"source":["def conv_block(in_f, out_f, *args, **kwargs):\n","    return nn.Sequential(\n","        nn.Conv2d(in_f, out_f, *args, **kwargs),\n","        nn.BatchNorm2d(out_f),\n","        nn.ReLU()\n","    )\n","class MyModule(nn.Module):\n","    def __init__(self, sizes):\n","        super().__init__()\n","        self.layers = nn.ModuleList([nn.Linear(in_f, out_f) for in_f, out_f in zip(sizes, sizes[1:])])\n","        self.trace = []\n","\n","    def forward(self,x):\n","        for layer in self.layers:\n","            x = layer(x)\n","            self.trace.append(x)\n","        return x\n","\n","model = MyModule([1, 16, 32])\n","import torch\n","model(torch.tensor([[0.23],[0.23],[0.23],[0.223]]))\n","print(model.layers)\n","[print(trace.shape) for trace in model.trace]\n"]},{"cell_type":"markdown","metadata":{"id":"VG2p-wFq5l2k"},"source":["#A Basic Backpropagation Example\n","\n","This example solves a binary classification problem by using 2 layers network. Please not that the problem is not linearly separable thus we cannot use a perceptron algorithm to solve it. \n","\n","1- Create a toy data and a network\n","\n","2- Plot it\n","\n","3- Calculate forward mode\n","\n","4- Calculate backward mode\n","\n","5- Train the network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hc2ZUlsw6IBY"},"outputs":[],"source":["# 1 - Create a toy data and a network\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.special import expit as sigmoid\n","import math \n","import random\n","\n","#Data\n","np.random.seed(0) # to have the same data for every run\n","def make_dataset(num_points):\n","    radius = 5\n","    data = []\n","    labels = []\n","    # Generate positive examples\n","    for i in range(num_points//2):\n","        # the radius of positive examples\n","        r = np.random.uniform(0,radius*0.5)\n","        angle = np.random.uniform(0, 2*math.pi)\n","        x = r * math.sin(angle)\n","        y = r * math.cos(angle)\n","        data.append([x,y])\n","        labels.append(1)\n","    for i in range(num_points//2):\n","        r = np.random.uniform(radius*0.5, radius*0.7)\n","        angle = np.random.uniform(0,2*math.pi)\n","        x = r * math.sin(angle)\n","        y = r * math.cos(angle)\n","        data.append([x,y])\n","        labels.append(0)\n","\n","    data = np.asarray(data)\n","    labels = np.asarray(labels)\n","    return data,labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJPq-zUXBPsv"},"outputs":[],"source":["num_data = 200\n","data, labels = make_dataset(num_data)\n","\n","plt.scatter(data[:num_data//2,0],data[:num_data//2,1],color = 'red')\n","plt.scatter(data[num_data//2:,0], data[num_data//2:,1], color = 'blue')\n","\n","print(\"Data has been created Now go to the step 2... \")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12XZitKPNILi"},"outputs":[],"source":["# 2- Create the Network\n","# need to define the size of the layers and initilize their parameters randomly.\n","\n","params = {} # dictionary type\n","params['W2'] = np.random.randn(3,2)\n","params['b2'] = np.zeros(3)\n","params['W3'] = np.random.randn(3)\n","params['b3'] = 0\n","\n","# W1 will be bias1 to vectorize the calculation."]},{"cell_type":"markdown","metadata":{"id":"Bnq__ujmRsnN"},"source":["Forward mode:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ttz69melRntK"},"outputs":[],"source":["# Forward mode\n","\n","z2 = np.dot(x,data)"]},{"cell_type":"markdown","metadata":{"id":"q6Kkr1ofiLm6"},"source":["## **AUTOGRAD**\n","code:https://www.youtube.com/watch?v=1fXB0Lc9RMI&list=PLLeO8f6PhlKb_FAC7qxOBtxT9-8EPDAqk&index=4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uw7SGpBjiPPM"},"outputs":[],"source":["import numpy as np\n","import torch\n","\n","x=torch.arange(4.0,requires_grad=True)\n","print(f\"x grad =\",x.grad,f\"\\nx  =\" , x)\n","\n","x_t=torch.transpose(x,0,0)\n","y=x@x_t    # y=torch.dot(x,x)\n","z=x*x      # element wise, z=1x4\n","\n","print(\"\\ntype y ={}, y={}, y.grad = {}\".format({type(y)}, {y}, {y.grad}))\n","\n","y.backward()\n","print(f\"\\nx grad with scaler output after backward=\",x.grad)\n","x.grad.zero_()\n","\n","z.backward(torch.tensor([1.0,1.0,1.0,1.0]))     # J_T x V \n","print(f\"\\nx grad with vector output after backward=\",x.grad)"]},{"cell_type":"markdown","metadata":{"id":"rpKriNnyRJgv"},"source":["# **MNIST_FEED_FORWARD**"]},{"cell_type":"markdown","metadata":{"id":"gX7I9GmSzy7N"},"source":["DATALOADER:\n","https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L09/code/custom-dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ND9unE67Cor6"},"outputs":[],"source":["from torchvision.datasets import MNIST\n","import torchvision.transforms as transforms\n","import numpy as np\n","import torch\n","import torch.nn as nn \n","import torch.nn.functional as F\n","from matplotlib import pyplot as plt\n","\n","dataset=MNIST(root='data/',download=True,transform=transforms.ToTensor())\n","print(type(dataset[0]))\n","\n","img_tensor,label=dataset[0]\n","print(img_tensor.shape,label)\n","\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.utils.data.dataloader import DataLoader\n","\n","def split_indices(n,val_pct):\n","  n_val=int(val_pct*n)\n","  idxs=np.random.permutation(n)\n","  ##  id=np.random.rand(5) between 0-1 with 5 elements \n","  return idxs[n_val:],idxs[:n_val]\n","\n","train_indices,val_indices=split_indices(len(dataset),val_pct=0.2)\n","print(train_indices[:3],val_indices[:3])\n","\n","input_size=28*28\n","num_classes=10\n","batch_size=100\n","\n","train_sampler=SubsetRandomSampler(train_indices)\n","train_loader=DataLoader(dataset,batch_size,sampler=train_sampler)\n","val_sampler=SubsetRandomSampler(val_indices)\n","val_loader=DataLoader(dataset,batch_size,sampler=val_sampler)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":513},"executionInfo":{"elapsed":738,"status":"ok","timestamp":1667641253009,"user":{"displayName":"Cihan Katar","userId":"05207696428297684225"},"user_tz":-180},"id":"zAFu29PeRjTS","outputId":"784b75ea-604f-424e-8438-648dd19a50ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["model\n"," MnistModel(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear1): Linear(in_features=784, out_features=512, bias=True)\n","  (linear2): Linear(in_features=512, out_features=10, bias=True)\n","  (softmax): LogSoftmax(dim=1)\n",")\n","image size :  torch.Size([100, 1, 28, 28])\n","model output size :  torch.Size([100, 10])\n","torch.Size([100, 10])\n","torch.Size([1, 10])\n","(28, 28)\n"]},{"data":{"text/plain":["Text(0.5, 1.0, ' model: T=%i')"]},"execution_count":27,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["class MnistModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.flatten=nn.Flatten()\n","    self.linear1=nn.Linear(28*28,512)\n","    self.linear2=nn.Linear(512,10)\n","    self.softmax=nn.LogSoftmax(dim=1)\n","    \n","  def forward(self, image):\n","    flatten=self.flatten(image)\n","    output=self.linear1(flatten)\n","    output=F.relu(output)\n","    output=self.linear2(output)\n","    return output\n","  def Softmax (self,output):\n","    return self.softmax(output)\n","\n","model=MnistModel()\n","\n","print(f'model\\n',model)\n","\n","'''\n","1)                                            2)\n","for images, labels in train_loader:           images, labels = next(iter(train_loader))\n","  print(images.shape)\n","  print(labels.shape)\n","  outputs=model(images)\n","  break\n","'''\n","images, labels = next(iter(train_loader))\n","\n","print(f'image size : ',images.shape)\n","print(f'model output size : ',model(images).shape)\n","'''\n","for name, param in model.named_parameters():      #printing all model parameters\n","  print(\"\\n\",name, param)\n","'''\n","pred_probab=model.softmax(model(images))    \n","pred_probab2=model.softmax(model(images[0]))    \n","print(pred_probab.shape)\n","print(pred_probab2.shape)\n","print(images[0].numpy().squeeze().shape)\n","\n","plt.imshow(images[0].numpy().squeeze())\n","\n","plt.title(' model: T=%i')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":59},"executionInfo":{"elapsed":859,"status":"ok","timestamp":1667641259311,"user":{"displayName":"Cihan Katar","userId":"05207696428297684225"},"user_tz":-180},"id":"FoWG4ZUjAzq5","outputId":"c225adbc-502c-4969-e296-9ed15047f222"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 10 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["figure = plt.figure()\n","num_of_images = 10\n","for index in range(1, num_of_images + 1):\n","    plt.subplot(6, 10, index)\n","    plt.axis('off')\n","    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')"]},{"cell_type":"markdown","metadata":{"id":"6bkAfept5oPS"},"source":["# **LOSS_FUNCTION**\n","code:https://www.youtube.com/playlist?list=PLE3Y6O9R81ly4UmavfbfaRKbih605E_uG"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1667641260875,"user":{"displayName":"Cihan Katar","userId":"05207696428297684225"},"user_tz":-180},"id":"sDkTKRvkaywc","outputId":"6d50b7a2-a4f0-481f-bf29-dc4deacf4fd4"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([100, 1, 28, 28])\n","torch.Size([1, 10])\n","tensor(2.2479, grad_fn=<NllLossBackward0>)\n","linear1.weight torch.Size([512, 784])\n","linear1.bias torch.Size([512])\n","linear2.weight torch.Size([10, 512])\n","linear2.bias torch.Size([10])\n"]}],"source":["criterion = nn.NLLLoss()                   # if cross entropy is used then --> loss_fn_ce = nn.CrossEntropyLoss()  ---> includes softmax .\n","#images = images.view(images.shape[0], -1) # flatten is defined in MnistModel class. it is not necessery to define here\n","print(images.shape)\n","neural_net_output=model(images[2])            #size of nn output = 100x10\n","logps = model.Softmax(neural_net_output)      #log probabilities\n","loss  = criterion(logps, labels[2:3])           #calculate the NLL loss\n","print(logps.shape)\n","print(loss)\n","\n","a=[]\n","for name, param in model.named_parameters():      #printing all model parameters\n","  a.append(param)\n","  print(name,param.shape)\n","\n","#print(a[0].shape)   #1. weights\n","#print(a[1].shape)   #1. bias\n","#print(a[2].shape)   #2 weights\n","#print(a[3].shape)   #2. bias"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1667641261923,"user":{"displayName":"Cihan Katar","userId":"05207696428297684225"},"user_tz":-180},"id":"MKkX2RwHe-Ud","outputId":"1b72d756-75db-4bc7-d495-813cb1136146"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n"]}],"source":["print(type(labels[0:1]))\n","print(type(labels[0])) "]},{"cell_type":"markdown","metadata":{"id":"T2KSClde7TXf"},"source":["# **OPTIMIZER**\n","code:https://www.youtube.com/playlist?list=PLE3Y6O9R81ly4UmavfbfaRKbih605E_uG"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_31ETBMrMabN"},"outputs":[],"source":["from time import time\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n","criterion = nn.NLLLoss()                   # loss_fn_ce = nn.CrossEntropyLoss()  ---> includes softmax .\n","time0 = time()\n","epochs = 10\n","for e in range(epochs):\n","    running_loss = 0\n","    for images, labels in train_loader:\n","        # Flatten MNIST images into a 784 long vector\n","        # images = images.view(images.shape[0], -1)\n","    \n","        # Training pass\n","        optimizer.zero_grad()\n","        \n","        neural_net_output=model(images)            #size of nn output = 100x10\n","        logps = model.Softmax(neural_net_output)   #log probabilities\n","        loss = criterion(logps, labels)\n","        \n","        #This is where the model learns by backpropagating\n","        loss.backward()\n","        \n","        #And optimizes its weights here\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","    else:\n","        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(train_loader)))\n","print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"]},{"cell_type":"markdown","metadata":{"id":"cbh2bdeCO-4X"},"source":["# **OPTIMIZER MANUEL**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBmM3QsI7W8J"},"outputs":[],"source":["import torch\n","from torch.nn import MSELoss, Parameter\n","\n","# The data function is: y = x + 10\n","x_train = torch.tensor([1, 2, 3, 4])\n","y_train = torch.tensor([11, 12, 13, 14], dtype=torch.float)\n","print(x_train, y_train)\n","\n","# Simple Linear Regression: a + bx\n","a = torch.randn(1, requires_grad=True, dtype=torch.float)\n","b = torch.randn(1, requires_grad=True, dtype=torch.float)\n","lr = 0.1\n","# If we use nn.Module to create a model, it will model.parameters()\n","model = [Parameter(a.clone()), Parameter(b.clone())]\n","\n","criterion = MSELoss()\n","optimizer = torch.optim.SGD(model, lr=lr)\n","# optimizer = torch.optim.Adam(model, lr=lr)\n","\n","for epoch in range(5):\n","    # Remove the grad computed in the last step\n","    optimizer.zero_grad()\n","    print(f'model params')\n","    print(model[0])\n","    print(a)\n","    print(model[1])\n","    print(b)\n","\n","    # Run a + bx\n","    y_predicted = model[0] + model[1] * x_train\n","    loss = criterion(y_predicted, y_train)\n","    loss.backward()\n","    optimizer.step()\n","\n","    with torch.no_grad():\n","        # Let us compute manually\n","        y_predicted_manually = a + b * x_train\n","        error = (y_train - y_predicted_manually)\n","        loss_manual = (error ** 2).mean()\n","        # Computes gradients for both \"a\" and \"b\" parameters\n","        a_grad = -2 * error.mean()\n","        b_grad = -2 * (x_train * error).mean()\n","\n","        # Updates parameters using gradients and the learning rate\n","        a = a - lr * a_grad\n","        b = b - lr * b_grad\n","\n","print(f'model predictions')\n","print(y_predicted)\n","print(y_predicted_manually)\n","\n","print(f'params grad')\n","print(model[0].grad)\n","print(a_grad)\n","print(model[1].grad)\n","print(b_grad)\n","\n","print(f'model loss')\n","print(loss)\n","print(loss_manual)"]},{"cell_type":"markdown","metadata":{"id":"bdK-YS15RXFr"},"source":["## **TEST & ACCURACY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvVAk1cyRZjC"},"outputs":[],"source":["correct_count, all_count = 0, 0\n","for images,labels in val_loader:\n","  for i in range(len(labels)):\n","    img = images[i]\n","    with torch.no_grad():\n","        neural_net_output=model(img)                #size of nn output = 100 x (1x10)  \n","        logps = model.Softmax(neural_net_output)    #size of nn output = 100 x (1x10)  \n","\n","    ps = torch.exp(logps)\n","    probab = list(ps.numpy()[0])\n","    pred_label = probab.index(max(probab))\n","    true_label = labels.numpy()[i]\n","    if(true_label == pred_label):\n","      correct_count += 1\n","    all_count += 1\n","\n","print(\"Number Of Images Tested =\", all_count)\n","print(\"\\nModel Accuracy =\", (correct_count/all_count))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbXk_emLS0dN"},"outputs":[],"source":["\n","print(ps)\n","probab = list(ps.numpy()[0])\n","\n","\n","plt.imshow(images[0].numpy().squeeze())"]},{"cell_type":"markdown","metadata":{"id":"jr2lroEhg2LM"},"source":["# **OOP_Torch_passing_forward**\n","code: https://discuss.pytorch.org/t/can-forward-in-nn-module-be-override-with-different-arguments/10840/4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcDrxGzog4qC"},"outputs":[],"source":["class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.fc = nn.Linear(10, 2)\n","        \n","    def forward(self, x):\n","        if self.training:     # flag is True then return X\n","            return x\n","        else:\n","            return self.fc(x)\n","\n","x = torch.randn(1, 10)        \n","model = MyModel()\n","print(model(x).shape)\n","\n","model.eval()           #false flag\n","print(model(x).shape)"]},{"cell_type":"markdown","metadata":{"id":"FeNeH1R3oWD8"},"source":["# **CNN BASIC EXAMPLE**\n","CODE:https://www.youtube.com/watch?v=wnK3uWv_WkU&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCsjEzJWoedc"},"outputs":[],"source":["# Imports\n","import torch\n","import torchvision # torch package for vision related things\n","import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n","import torchvision.datasets as datasets  # Standard datasets\n","import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n","from torch import optim  # For optimizers like SGD, Adam, etc.\n","from torch import nn  # All neural network modules\n","from torch.utils.data import DataLoader  # Gives easier dataset managment by creating mini batches etc.\n","from tqdm import tqdm  # For nice progress bar!\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","in_channels = 1\n","num_classes = 10\n","learning_rate = 0.001\n","batch_size = 64\n","num_epochs = 3\n","\n","# Load Data\n","train_dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True)\n","test_dataset = datasets.MNIST(root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9JOD8N48oaU5"},"outputs":[],"source":["# Simple CNN\n","class CNN(nn.Module):\n","    def __init__(self, in_channels=1, num_classes=10):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_channels=in_channels,\n","            out_channels=8,\n","            kernel_size=(3, 3),\n","            stride=(1, 1),\n","            padding=(1, 1),\n","        )\n","        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n","        self.conv2 = nn.Conv2d(\n","            in_channels=8,\n","            out_channels=16,\n","            kernel_size=(3, 3),\n","            stride=(1, 1),\n","            padding=(1, 1),\n","        )\n","        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = x.reshape(x.shape[0], -1)\n","        x = self.fc1(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cjV9xhRZopoS"},"outputs":[],"source":["# Initialize network\n","model = CNN(in_channels=in_channels, num_classes=num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BK4Et9Tyoke3"},"outputs":[],"source":["\n","# Train Network\n","for epoch in range(num_epochs):\n","    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n","        # Get data to cuda if possible\n","        data = data.to(device=device)\n","        targets = targets.to(device=device)\n","\n","        # forward\n","        scores = model(data)\n","        loss = criterion(scores, targets)\n","\n","        # backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        # gradient descent or adam step\n","        optimizer.step()\n","\n","# Check accuracy on training & test to see how good our model\n","def check_accuracy(loader, model):\n","    num_correct = 0\n","    num_samples = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device=device)\n","            y = y.to(device=device)\n","\n","            scores = model(x)\n","            _, predictions = scores.max(1)\n","            num_correct += (predictions == y).sum()\n","            num_samples += predictions.size(0)\n","\n","\n","    model.train()\n","    return num_correct/num_samples\n","\n","\n","print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n","print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"rwgk8c-Xxg4Q"},"source":["# ***UNET FOR MEDICAL IMAGE SEGMENTATAION***\n","code:https://www.youtube.com/watch?v=IHq1t7NxS8k&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=42"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slJRthXIxtip"},"outputs":[],"source":["import numpy as np\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn \n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import ToTensor\n","\n","class DoubleConv(nn.Module):\n","  def __init__(self,in_channels,out_channels):\n","    super(DoubleConv, self).__init__()\n","    self.conv=nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels,3,1,1,bias=False),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_channels, out_channels,3,1,1,bias=False),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","    )\n","  def forward (self,x):\n","    return self.conv(x)\n","\n","class UNET(nn.Module):\n","  def __init__(self,in_channels,out_channels, features=[64,256,512]):\n","    super(UNET,self).__init__()\n","    self.ups=nn.ModuleList()\n","    self.downs=nn.ModuleList()\n","    self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n","\n","    for feature in features:\n","      self.downs.append(DoubleConv(in_channels,feature))\n","      in_channels = feature\n","\n","    for feature in reversed(features):\n","      self.ups.append(nn.ConvTranspose2d(feature*2,feature,kernel_size=2,stride=2))\n","      self.ups.append(DoubleConv(feature*2,feature))\n","\n","    self.bottleneck = DoubleConv(features[-1], features[-1*2])\n","    self.final_conv = nn.Conv2d(features[0], out_channels)\n","  \n","  def forward(self, x):\n","    skip_connections=[]\n","    for down in self.downs:\n","      x = down(x)\n","      skip_connections.append(x)\n","      x = self.pool(x)\n","    \n","    x = self.bottleneck(x)\n","    skip_connections = skip_connections[::-1]\n","    \n","    for idx in range(0,len(self.ups),2):\n","      x=self.ups[idx](x)\n","      skip_connection=skip_connections[idx//2]\n","      concat_skip = torch.cat((skip_connection, x), dim=-1)\n","      x = self.ups[idx+1](concat_skip)\n","    \n","    return self.final_conv(x)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"}}},"nbformat":4,"nbformat_minor":0}
